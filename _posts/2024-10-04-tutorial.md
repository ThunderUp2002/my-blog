---
layout: post
title:  "Predicting the Future with Linear Regression"
date: 2024-10-04
author: Devan Gwynn
description: "A tutorial on linear regression, including exploratory data analysis, variable selection, and making predictions"
---

<p class="intro"><span class="dropcap">H</span>ave you ever wanted to predict the future? Well, using linear regression, you can! In this post, I will walk through the steps to construct a linear model in R and then use that model to make predictions.</p>

### What is linear regression?

Linear regression is a method used in statistics to model a linear relationship between a dependent variable and one or more independent variables, which can then be used to make predictions about future data. Thus, it is considered one of the simplest forms of supervised machine learning, in which the model learns by evaluating labeled data. When more than one predictor variable is used, it is called multiple linear regression, but the same ideas apply as in the simple (single predictor variable) case. The model is constructed by calculating coefficients for each predictor that will minimize the error between its prediction for the response variable and the actual value. There are a lot of mathematical concepts behind this idea that I will not go over, but more information on linear regression as a whole can be found <a href="https://en.wikipedia.org/wiki/Linear_regression" target="_blank">here</a>.

### Multiple linear regression steps

I will cover the following steps in constructing a multiple linear regression model:
1. Exploratory data analysis (EDA)
2. Fit the linear model
3. Perform variable selection
4. Check assumptions
5. Use the model for inference and predictions

I will use R to demonstrate each of these tasks, but there are many other ways to accomplish them (such as using Python). Additionally, I will be using data has already been cleaned and read in as the object "bodyfat." The data consists of a body fat measurement (known as brozek), age, height, weight, and neck, chest, and abdomen fat measurements. Our goal is to predict brozek using the other variables provided. Let's get into it!

### Step 1: Exploratory data analysis (EDA)

The first step in linear regression (and almost any data science project) is exploratory data analysis (EDA). This is where we will produce various numerical summaries, visualizations, etc. to get familiar with the data and examine any preliminary relationships we can discover through observation. There are many different things that are helpful to look at during the EDA process, and it largely depends on what types of variables (quantitative vs. categorical) you are comparing. I will provide a few examples, but they are by no means exhaustive since EDA is all about discovering the data for yourself.

The following code:
```
summary(bodyfat)
```
will generate a summary of the data:
<figure>
	<img src="{{site.url}}/{{site.baseurl}}/assets/img/2024-10-04-tutorial/bodyfat-summary.jpg" alt=""> 
	<figcaption>Summary of the body fat data</figcaption>
</figure>

The following code:
```
pairs(bodyfat)
```
will generate a scatterplot matrix:
<figure>
	<img src="{{site.url}}/{{site.baseurl}}/assets/img/2024-10-04-tutorial/bodyfat-scatterplot-matrix.png" alt=""> 
	<figcaption>Scatterplot matrix of the body fat data</figcaption>
</figure>

The following code (after installing and loading the corrplot package):
```
corrplot(cor(bodyfat))
```
will generate a correlation matrix:
<figure>
	<img src="{{site.url}}/{{site.baseurl}}/assets/img/2024-10-04-tutorial/bodyfat-correlation-matrix.png" alt=""> 
	<figcaption>Correlation matrix of the body fat data</figcaption>
</figure>

### Step 2: Fit the linear model

After taking note of interesting relationships among variables in the data, we can now move on to fitting the linear model. The code to do this is actually quite simple:
```
bodyfat_lm <- lm(brozek ~ age + weight + height + neck + chest + abdom, data = bodyfat)
```

The coefficients, along with some additional useful information, can be accessed by calling the summary function on the model object just created:
```
summary(bodyfat_lm)
```
giving the following output:
<figure>
	<img src="{{site.url}}/{{site.baseurl}}/assets/img/2024-10-04-tutorial/bodyfat-model-summary.jpg" alt=""> 
	<figcaption>Body fat model summary</figcaption>
</figure>

### Step 3: Perform variable selection

Once we have our initial model, how do we know which of our predictor variables to include? There are several different variable selection procedures that can be used to determine this.

#### Variable selection methods
1. **Best subsets** considers all possible subsets of the variables and selects the one with the best value for a given selection criteria <a href="https://www.statology.org/best-subset-selection/#:~:text=Best%20subset%20selection%20offers%20the%20following%20pros:%20It%E2%80%99s" target="_blank">(More info)</a>
2. **Stepwise** <a href="https://www.statology.org/stepwise-selection/#:~:text=An%20alternative%20to%20best%20subset%20selection%20is%20known" target="_blank">(More info)</a>
    * *Forward selection* starts with an empty set of variables and progressively adds variables to the model one by one based on what improves a given selection criteria the most
    * *Backward selection* starts with the full set of variables and progressively removes variables from the model one by one based on what improves a given selection criteria the most
3. **Shrinkage methods**
    * *Ridge* uses an L2 penalty term that "shrinks" variable coefficients* <a href="https://www.statology.org/ridge-regression/" target="_blank">(More info)</a>
    * *LASSO* uses an L1 penalty term that "shrinks" variable coefficients (sometimes down to zero) <a href="https://www.statology.org/lasso-regression/" target="_blank">(More info)</a>
    * *Elastic net* is a combination of ridge and LASSO <a href="https://en.wikipedia.org/wiki/Elastic_net_regularization" target="_blank">(More info)</a>

*Ridge regression is technically not a variable selection method since it shrinks coefficients but does not reduce any to zero.

I will use elastic net regression to demonstrate variable selection, but I have provided a link with more information for each of the methods listed above. Elastic net regression is essentially a blend between ridge and LASSO regression in which the penalty terms for each is combined. There is once again a mathematical foundation here, but for our purposes, it suffices to say the penalty term determines which coefficients should be shrunk and by how much, which sometimes results in a predictor variable being dropped from the model altogether.

The following code performs elastic net regularization and extracts the new coefficients (after having separated the predictors and the response variable into separate matrices):
```
bodyfat_elastic_cv <- cv.glmnet(x = bodyfat_x,
                                y = bodyfat_y, 
                                type.measure = "mse", 
                                alpha = 0.5)

coef(bodyfat_elastic_cv, s = "lambda.1se")
```

The output will vary slightly since this method uses cross-validation, a process that incorporates randomness:
<figure>
	<img src="{{site.url}}/{{site.baseurl}}/assets/img/2024-10-04-tutorial/bodyfat-elastic-net-coefficients.jpg" alt=""> 
	<figcaption>Body fat elastic net coefficients</figcaption>
</figure>

Variables with a . next to them indicate they have been reduced to zero and thus should be removed from the model. We will now refit the model with the variables that have been selected and provide a summary of the new model:

```
bodyfat_final_lm <- lm(brozek ~ age + height + abdom, data = bodyfat)
summary(bodyfat_final_lm)
```

<figure>
	<img src="{{site.url}}/{{site.baseurl}}/assets/img/2024-10-04-tutorial/bodyfat-final-model-summary.jpg" alt=""> 
	<figcaption>Body fat final model summary</figcaption>
</figure>

### Step 4: Check assumptions

As with most statistical models, the multiple linear regression model comes with some assumptions.

#### Linear regression assumptions
<dl>
  <dt>Linearity</dt>
  <dd>The relationship between the response variable and predictor variable(s) is linear</dd>
  <dt>Independence</dt>
  <dd>The errors are independent from each other</dd>
  <dt>Normality</dt>
  <dd>The errors are distributed normally</dd>
  <dt>Equal variance</dt>
  <dd>The errors have constant variance across all values of the predictor variables</dd>
  <dt>No multicollinearity</dt>
  <dd>None of the predictor variables are strongly correlated with each other</dd>
  <dt>No influential points</dt>
  <dd>There are no points that strongly affect the model's predictions</dd>
</dl>

Since the main point of this post is using linear regression for predictions, we will assume these assumptions have already been checked for the sake of simplicity. More information on the process of checking these assumptions can be found <a href="https://www.statology.org/multiple-linear-regression-assumptions/#:~:text=Multiple%20linear%20regression%20is%20a%20statistical%20method%20we" target="_blank">here</a>.

### Step 5: Use the model for inference and predictions

Now, the moment you've all been waiting for--predicting the future! Once we have checked the validity of the model's assumptions, we can now use it for inference and predictions. I first want to provide a brief explanation of the difference between confidence intervals and prediction intervals:
<dl>
  <dt>95% confidence interval</dt>
  <dd>Provides a range of values that we are 95% confident contains the true mean of the response variable for specific values of the explanatory variables</dd>
  <dt>95% prediction interval</dt>
  <dd>Provides a range of values that we are 95% confident contains the true value of the response variable for specific values of the explanatory variables</dd>
</dl>

Therefore, a prediction interval will always be wider than its corresponding confidence interval because there is more uncertainty.

The following code provides a 95% confidence interval for hypothetical values of the explanatory variables:
```
predict(bodyfat_final_lm,
        newdata = data.frame(age = 22,
                             weight = 150,
                             height = 72,
                             neck = 34,
                             chest = 94,
                             abdom = 86),
        interval = "confidence",
        level = 0.95)
```

<figure>
	<img src="{{site.url}}/{{site.baseurl}}/assets/img/2024-10-04-tutorial/bodyfat-confidence-interval.jpg" alt=""> 
	<figcaption>Body fat confidence interval</figcaption>
</figure>

This means we are 95% confident the true mean brozek value is between 12.27 and 14.43 for the given values of explanatory variables.

The following code provides a 95% prediction interval (for the same values of explanatory variables as the previous confidence interval):
```
predict(bodyfat_final_lm,
        newdata = data.frame(age = 22,
                             weight = 150,
                             height = 72,
                             neck = 34,
                             chest = 94,
                             abdom = 86),
        interval = "prediction",
        level = 0.95)
```

<figure>
	<img src="{{site.url}}/{{site.baseurl}}/assets/img/2024-10-04-tutorial/bodyfat-prediction-interval.jpg" alt=""> 
	<figcaption>Body fat prediction interval</figcaption>
</figure>

This means we are 95% confident the true brozek value is between 5.13 and 21.57 for an individual with the given values of explanatory variables.

In addition to these predictions, we can also make inferences on the coefficients (indicating each predictor's effect on the response variable with a one unit increase, holding all others constant) and their significance (based on each predictor's p-value).

### Summary

Linear regression is a very useful tool that can be utilized to model the relationship between a response variable and one or more predictor variables. After investigating relationships between variables in the dataset, we construct a linear model and perform variable selection to determine which predictor variables are most beneficial to use in the model. We then check assumptions to ensure the model is valid before using it for inference and predictions. With more data than ever before, there are so many different applications you can use linear regression for. From sports to music, it can be used with almost any dataset in some way. What will you do with your newfound knowledge of linear regression? The future is yours to predict.